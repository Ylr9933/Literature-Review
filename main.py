# main.py

import os
os.environ["http_proxy"] = "http://127.0.0.1:7890"
os.environ["https_proxy"] = "http://127.0.0.1:7890"
import arxiv
import requests
import streamlit as st
from langchain_core.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
import dashscope
from dashscope import Generation
import pandas as pd
import matplotlib.pyplot as plt
import networkx as nx
from datetime import datetime
import re
import PyPDF2
import io
import base64
from utils.logger import Logger
# è®¾ç½®é˜¿é‡Œäº‘APIå¯†é’¥
dashscope.api_key = "sk-ceff4d200590488f98d96d3b95e915f0"  # æ›¿æ¢ä¸ºä½ è‡ªå·±çš„ DashScope API Key
def call_qwen(prompt):
    response = Generation.call(
        model="qwen-max",  # å¯ä»¥æ¢æˆ qwen-plus æˆ– qwen-turbo
        prompt=prompt
    )
    return response.output.text

# åˆ›å»ºæ–‡çŒ®æ£€ç´¢Agent
class LiteratureSearchAgent:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
    
    def search_arxiv(self, topic, max_results=10):
        """æœç´¢arXivè®ºæ–‡"""
        client = arxiv.Client()
        search = arxiv.Search(
            query=topic,
            max_results=max_results,
            sort_by=arxiv.SortCriterion.SubmittedDate
        )
        
        papers = []
        for result in client.results(search):
            paper_info = {
                "title": result.title,
                "authors": ", ".join(author.name for author in result.authors),
                "summary": result.summary,
                "url": result.pdf_url,
                "published": result.published.strftime("%Y-%m-%d"),
                "code_link": None
            }
            papers.append(paper_info)
        
        return papers
    
    def find_github_code(self, topic, num_repos=5):
        """åœ¨GitHubä¸Šæœç´¢ç›¸å…³ä»£ç """
        url = f"https://api.github.com/search/repositories?q={topic}&sort=stars&order=desc"
        response = requests.get(url, headers=self.headers)
        
        if response.status_code == 200:
            repos = response.json().get('items', [])
            code_links = []
            
            for repo in repos[:num_repos]:
                code_info = {
                    "name": repo['full_name'],
                    "description": repo['description'] or "",
                    "url": repo['html_url'],
                    "stars": repo['stargazers_count']
                }
                code_links.append(code_info)
            
            return code_links
        else:
            st.error(f"GitHubæœç´¢å¤±è´¥: {response.status_code}")
            return []

# åˆ›å»ºæ–‡çŒ®åˆ†æAgent
class LiteratureAnalysisAgent:
    def __init__(self):
        self.embeddings = HuggingFaceEmbeddings(
            model_name="shibing624/text2vec-base-chinese"
        )
        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        
    def process_papers(self, papers):
        """å¤„ç†è®ºæ–‡æ•°æ®å¹¶åˆ›å»ºå‘é‡æ•°æ®åº“"""
        documents = []
        
        for paper in papers:
            content = f"Title: {paper['title']}\nAuthors: {paper['authors']}\nPublished: {paper['published']}\nSummary: {paper['summary']}"
            chunks = self.text_splitter.split_text(content)
            documents.extend(chunks)
        
        vectorstore = FAISS.from_texts(documents, self.embeddings)
        return vectorstore
    
    def generate_summary(self, papers):
        """ç”Ÿæˆæ–‡çŒ®ç»¼è¿°"""
        prompt_template = """
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç§‘ç ”åŠ©æ‰‹ï¼Œéœ€è¦æ ¹æ®ä»¥ä¸‹è®ºæ–‡ä¿¡æ¯æ’°å†™æ–‡çŒ®ç»¼è¿°ï¼š
        
        è®ºæ–‡ä¿¡æ¯ï¼š
        {papers}
        
        è¯·æŒ‰ç…§ä»¥ä¸‹ç»“æ„æ’°å†™ç»¼è¿°ï¼š
        1. ä¸»è¦ç ”ç©¶é¢†åŸŸæ¦‚è¿°
        2. å…³é”®åˆ›æ–°ç‚¹åˆ†æ
        3. ä½¿ç”¨çš„æ•°æ®é›†å’Œå®éªŒç»“æœ
        4. ä¸åŒè®ºæ–‡ä¹‹é—´çš„è”ç³»å’Œå‘å±•
        
        ç»¼è¿°å†…å®¹åº”å‡†ç¡®ã€ä¸“ä¸šï¼Œé¿å…äº§ç”Ÿå¹»è§‰æˆ–é”™è¯¯ä¿¡æ¯ã€‚
        """
        
        papers_str = ""
        for i, paper in enumerate(papers, 1):
            papers_str += f"{i}. {paper['title']} ({paper['published']})\n"
            papers_str += f"   æ‘˜è¦: {paper['summary']}\n\n"
        
        prompt = PromptTemplate(template=prompt_template, input_variables=["papers"])
        final_prompt = prompt.format(papers=papers_str)
        
        summary = call_qwen(final_prompt)
        return summary
    
    def create_evolution_graph(self, papers):
        """åˆ›å»ºç®—æ³•å‘å±•æ¼”è¿›å›¾"""
        G = nx.DiGraph()
        
        # æå–è®ºæ–‡å¹´ä»½å’Œæ ‡é¢˜
        paper_years = []
        for paper in papers:
            year = datetime.strptime(paper['published'], "%Y-%m-%d").year
            paper_years.append((year, paper['title']))
        
        # æŒ‰æ—¶é—´æ’åº
        paper_years.sort()
        
        # åˆ›å»ºèŠ‚ç‚¹å’Œè¾¹
        for year, title in paper_years:
            G.add_node(title, year=year)
        
        nodes = list(G.nodes)
        for i in range(len(nodes) - 1):
            G.add_edge(nodes[i], nodes[i + 1])
        
        # ç»˜åˆ¶å›¾è¡¨
        plt.figure(figsize=(12, 8))
        pos = nx.spring_layout(G, seed=42)
        nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=3000, font_size=10, font_weight='bold')
        nx.draw_networkx_edges(G, pos, arrows=True)
        
        edge_labels = {(u, v): d['weight'] if 'weight' in d else '' for u, v, d in G.edges(data=True)}
        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)
        
        plt.title("ç®—æ³•å‘å±•æ¼”è¿›å›¾")
        plt.axis('off')
        
        # ä¿å­˜ä¸ºå­—èŠ‚æµ
        buf = io.BytesIO()
        plt.savefig(buf, format="png", dpi=300, bbox_inches="tight")
        plt.close()
        buf.seek(0)
        
        return buf.getvalue()

# åˆ›å»ºæµ·æŠ¥ç”Ÿæˆå™¨
class PosterGenerator:
    def generate_poster(self, summary, graph_image, papers):
        """ç”ŸæˆåŒ…å«ç»¼è¿°å’Œæ¼”è¿›å›¾çš„æµ·æŠ¥"""
        from reportlab.lib.pagesizes import letter
        from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image, PageBreak
        from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
        from reportlab.pdfbase import pdfmetrics
        from reportlab.pdfbase.ttfonts import TTFont
        from reportlab.lib.enums import TA_CENTER, TA_LEFT, TA_RIGHT, TA_JUSTIFY
        # æ³¨å†Œä¸­æ–‡å­—ä½“
        font_path = os.path.join(os.path.dirname(__file__), "fonts", "simsun.ttc")
        if os.path.exists(font_path):
            pdfmetrics.registerFont(TTFont('SimSun', font_path))
        else:
            raise FileNotFoundError(f"å­—ä½“æ–‡ä»¶æœªæ‰¾åˆ°: {font_path}")

        # åˆ›å»ºPDFæ–‡æ¡£
        buffer = io.BytesIO()
        doc = SimpleDocTemplate(buffer, pagesize=letter)
        
        styles = getSampleStyleSheet()
        
        # è‡ªå®šä¹‰æ ·å¼ï¼ˆä½¿ç”¨ä¸­æ–‡å­—ä½“ï¼‰
        styles.add(ParagraphStyle(name='Chinese', fontName='SimSun', fontSize=12, leading=14))
        styles.add(ParagraphStyle(name='ChineseTitle', fontName='SimSun', fontSize=18, leading=22, alignment=TA_CENTER))
        styles.add(ParagraphStyle(name='ChineseHeading2', fontName='SimSun', fontSize=14, leading=16, bold=True))

        story = []

        # æ·»åŠ æ ‡é¢˜
        story.append(Paragraph("æ–‡çŒ®ç»¼è¿°æµ·æŠ¥", styles['ChineseTitle']))
        story.append(Spacer(1, 24))

        # æ·»åŠ æ¼”è¿›å›¾
        story.append(Paragraph("ç®—æ³•å‘å±•æ¼”è¿›å›¾", styles['ChineseHeading2']))
        img = io.BytesIO(graph_image)
        image = Image(img)
        image.drawHeight = 400
        image.drawWidth = 600
        story.append(image)
        story.append(Spacer(1, 12))

        # æ·»åŠ ç»¼è¿°å†…å®¹
        story.append(Paragraph("æ–‡çŒ®ç»¼è¿°", styles['ChineseHeading2']))
        paragraphs = summary.split('\n\n')
        for para in paragraphs:
            # ä½¿ç”¨ä¸­æ–‡æ ·å¼
            story.append(Paragraph(para, styles['Chinese']))
            story.append(Spacer(1, 6))

        # æ·»åŠ å‚è€ƒæ–‡çŒ®
        story.append(Paragraph("å‚è€ƒæ–‡çŒ®", styles['ChineseHeading2']))
        for i, paper in enumerate(papers, 1):
            ref_text = f"[{i}] {paper['title']} ({paper['published']})"
            story.append(Paragraph(ref_text, styles['Chinese']))

        # æ„å»ºæ–‡æ¡£
        doc.build(story)

        return buffer.getvalue()
# åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
logger = Logger(save_dir="logs", fname="streamlit_app.log")

import streamlit as st
from utils.logger import Logger

logger = Logger(save_dir="logs", fname="streamlit_app.log")
def main():
    logger.log("ã€å¯åŠ¨ã€‘Streamlit åº”ç”¨å¼€å§‹è¿è¡Œ")

    st.set_page_config(page_title="æ–‡çŒ®ç»¼è¿°Agent", page_icon="ğŸ“š", layout="wide")
    st.title("ğŸ“š æ–‡çŒ®ç»¼è¿°Agentå·¥å…·")
    # åˆå§‹åŒ–Agent
    search_agent = LiteratureSearchAgent()
    analysis_agent = LiteratureAnalysisAgent()
    poster_generator = PosterGenerator()
    
    # åˆå§‹åŒ– session_state
    if 'papers' not in st.session_state:
        st.session_state.papers = None
    if 'summary' not in st.session_state:
        st.session_state.summary = None
    if 'evolution_graph' not in st.session_state:
        st.session_state.evolution_graph = None
    if 'poster_pdf' not in st.session_state:
        st.session_state.poster_pdf = None

    # ç”¨æˆ·è¾“å…¥
    col1, col2 = st.columns([3, 1])
    with col1:
        topic = st.text_input("è¯·è¾“å…¥è¦æœç´¢çš„æ–‡çŒ®ä¸»é¢˜:")
    with col2:
        num_papers = st.slider("è®ºæ–‡æ•°é‡", min_value=5, max_value=20, value=10)

    if st.button("å¼€å§‹æœç´¢ä¸åˆ†æ"):
        logger.log(f"ã€ç”¨æˆ·æ“ä½œã€‘ç‚¹å‡» 'å¼€å§‹æœç´¢ä¸åˆ†æ'ï¼Œä¸»é¢˜ä¸º '{topic}'ï¼Œæ•°é‡ {num_papers}")

        if not topic:
            st.warning("è¯·è¾“å…¥ä¸€ä¸ªæ–‡çŒ®ä¸»é¢˜!")
            return

        try:
            with st.spinner("æ­£åœ¨ä»arXivè·å–æœ€æ–°è®ºæ–‡..."):
                logger.log("ã€é˜¶æ®µã€‘å¼€å§‹ä» arXiv è·å–è®ºæ–‡")
                papers = search_agent.search_arxiv(topic, num_papers)
                st.session_state.papers = papers
                logger.log(f"ã€å®Œæˆã€‘æˆåŠŸè·å– {len(papers)} ç¯‡è®ºæ–‡")
                st.subheader("æ‰¾åˆ°çš„è®ºæ–‡:")
                for paper in papers:
                    st.markdown(f"**{paper['title']}** ({paper['published']})")
                    st.markdown(f"*æ‘˜è¦:* {paper['summary']}")
                    st.markdown(f"[PDFé“¾æ¥]({paper['url']})")
                    st.markdown("---")
        except Exception as e:
            logger.error(f"ã€é”™è¯¯ã€‘æœç´¢è®ºæ–‡å¤±è´¥: {e}")
            st.error("è®ºæ–‡æœç´¢å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–APIé…ç½®")
            return
                # æ˜¾ç¤ºè®ºæ–‡ä¿¡æ¯

        try:
            with st.spinner("æ­£åœ¨ç”Ÿæˆæ–‡çŒ®ç»¼è¿°..."):
                logger.log("ã€é˜¶æ®µã€‘å¼€å§‹ç”Ÿæˆæ–‡çŒ®ç»¼è¿°")
                summary = analysis_agent.generate_summary(papers)
                st.session_state.summary = summary
                logger.log("ã€å®Œæˆã€‘æ–‡çŒ®ç»¼è¿°ç”Ÿæˆå®Œæˆ")
        except Exception as e:
            logger.error(f"ã€é”™è¯¯ã€‘ç”Ÿæˆç»¼è¿°å¤±è´¥: {e}")
            st.error("æ–‡çŒ®ç»¼è¿°ç”Ÿæˆå¤±è´¥")
            return

        try:
            with st.spinner("æ­£åœ¨åˆ›å»ºç®—æ³•å‘å±•æ¼”è¿›å›¾..."):
                logger.log("ã€é˜¶æ®µã€‘å¼€å§‹ç»˜åˆ¶ç®—æ³•æ¼”è¿›å›¾")
                evolution_graph = analysis_agent.create_evolution_graph(papers)
                st.session_state.evolution_graph = evolution_graph
                logger.log("ã€å®Œæˆã€‘ç®—æ³•æ¼”è¿›å›¾ç»˜åˆ¶å®Œæˆ")
        except Exception as e:
            logger.error(f"ã€é”™è¯¯ã€‘ç»˜å›¾å¤±è´¥: {e}")
            st.error("ç®—æ³•æ¼”è¿›å›¾ç»˜åˆ¶å¤±è´¥")
            return

        try:
            with st.spinner("æ­£åœ¨ç”Ÿæˆæµ·æŠ¥..."):
                logger.log("ã€é˜¶æ®µã€‘å¼€å§‹ç”ŸæˆPDFæµ·æŠ¥")
                poster_pdf = poster_generator.generate_poster(summary, evolution_graph, papers)
                st.session_state.poster_pdf = poster_pdf
                logger.log("ã€å®Œæˆã€‘PDFæµ·æŠ¥ç”Ÿæˆå®Œæˆ")
        except Exception as e:
            logger.error(f"ã€é”™è¯¯ã€‘ç”Ÿæˆæµ·æŠ¥å¤±è´¥: {e}")
            st.error("æµ·æŠ¥ç”Ÿæˆå¤±è´¥")
            return

    # å±•ç¤ºç»“æœï¼ˆæ— è®ºæ˜¯å¦åˆšæ‰§è¡Œå®Œæœç´¢è¿˜æ˜¯ä» session_state ä¸­æ¢å¤ï¼‰
    if st.session_state.summary:
        st.subheader("ç”Ÿæˆçš„æ–‡çŒ®ç»¼è¿°:")
        st.write(st.session_state.summary)

    if st.session_state.evolution_graph:
        st.subheader("ç®—æ³•å‘å±•æ¼”è¿›å›¾")
        st.image(st.session_state.evolution_graph, use_container_width=True)

    if st.session_state.poster_pdf:
        col1, col2, col3 = st.columns(3)
        with col1:
            st.download_button(
                label="ä¸‹è½½æµ·æŠ¥",
                data=st.session_state.poster_pdf,
                file_name=f"literature_review_{topic.replace(' ', '_')}.pdf",
                mime="application/pdf"
            )
        with col2:
            st.download_button(
                label="ä¸‹è½½ç»¼è¿°æ–‡æœ¬",
                data=st.session_state.summary,
                file_name=f"summary_{topic.replace(' ', '_')}.txt",
                mime="text/plain"
            )
        with col3:
            import pandas as pd
            df = pd.DataFrame(st.session_state.papers)
            csv = df.to_csv(index=False)
            st.download_button(
                label="ä¸‹è½½è®ºæ–‡åˆ—è¡¨",
                data=csv,
                file_name=f"papers_{topic.replace(' ', '_')}.csv",
                mime="text/csv"
            )

    logger.log("ã€ç»“æŸã€‘å½“å‰ä»»åŠ¡å·²å®Œæˆ")

if __name__ == "__main__":
    main()